# ğŸ¢ End-to-End Data Engineering Pipeline for Merging an Acquired Company Into a Unified Databricks Lakehouse

This project demonstrates a **full production-grade data engineering workflow** built on **Databricks Lakehouse**, designed for a real-world scenario where a **parent company acquires a child company** and needs to **ingest, clean, transform, merge, and unify** the acquired companyâ€™s data with its existing enterprise analytics ecosystem.

The architecture supports:
- Multi-layer medallion design (Bronze â†’ Silver â†’ Gold)
- Unit catalogs for enterprise-wide governance
- Automated ingestion from S3 via LakeFlow Jobs
- Gold-layer merging between child company and parent company
- Centralized dashboarding and LLM-driven analytics using Databricks Genie

---

## ğŸ“Š Architecture Overview

![Architecture](resources/project_architecture.png)
![Dashbaord1](2_dashboarding/Dashbaord_p1.png)
![Dashbaord2](2_dashboarding/Dashbaord_p2.png))

---

## ğŸ§  Business Context

A **parent company** has recently **acquired a child company** whose raw data exists in **Amazon S3**, containing:
- Unstructured CSVs  
- Inconsistent schemas  
- Raw ERP extracts  
- Historical archives  

The goal is to onboard the acquired data into the **parent company's analytics system**, standardize formats, merge reference structures, and expose unified insights via **Databricks dashboards and Genie**.

This architecture solves the full set of challenges:
- Schema drift  
- Missing or inconsistent fields  
- Duplicated products, customers, and transactions  
- Conflicting business rules  
- Variations in naming and categorization  
- Scalability and governance across two orgs  

---

## ğŸ—ï¸ End-to-End Pipeline Breakdown

### **1. Raw Data Layer (Landing Zone â€” Amazon S3)**  
The acquired company's raw data lands in S3:
- Sales exports  
- Product lists  
- Pricing sheets  
- Customer files  
- Raw logs and archives  

This data is **unstructured and inconsistent**, requiring ingestion and schema normalization.

### **2. LakeFlow Jobs â€” Automated Ingestion**
Databricks LakeFlow orchestrates:
- Auto-ingest from S3  
- File-based triggers  
- Schema inference and evolution  
- Deduplication  
- Raw data tracking  

Jobs run on a schedule to pull new files from S3 â†’ Bronze.

---

## ğŸ¥‰ **Bronze Layer â€” Raw Data**
The Bronze tables store:
- Row-level raw data  
- Minimal processing  
- File metadata (_filename, _ingest_time)  

Purpose:
- Preserve original history  
- Handle ingestion consistency  
- Enable replay and debugging  

---

## ğŸ¥ˆ **Silver Layer â€” Cleaned & Curated Data**
Transformation includes:
- Data type standardization  
- Null handling  
- Surrogate key creation  
- Category mapping  
- Customer code unification  
- Pricing standardization (INR normalization)  
- Reference table joining  

Outcome:
- Analytics-ready data  
- Fully consistent schemas between companies  

---

## ğŸ¥‡ **Gold Layer â€” Aggregated Analytics**
Gold tables represent:
- Fact sales tables  
- Dimensional models (dim_product, dim_customer)  
- Aggregations (daily, monthly, quarterly metrics)  
- Business KPI-level rollups  

This is where the **acquired companyâ€™s Gold layer merges with the parent companyâ€™s Gold layer**, forming a **unified enterprise analytics dataset**.

---

## ğŸ”— **Merging Child Company Data With Parent Company Data**

The child companyâ€™s gold datasets are aligned to the parent companyâ€™s business conventions:

- Category standardization  
- Platform mapping  
- Unified customer segments  
- Harmonized fiscal calendar  
- Product reference merging via SKUs  
- Duplicate entity resolution  

A new **Parent Org Gold Analytics Table** is generated, representing combined sales and product data.

This unified table powers company-wide analytics & reporting.

---

## ğŸ“Š Serving Layer â€” Dashboards & Genie

The parent company uses:
- **Databricks Dashboards**  
- **Databricks Genie (LLM analytics)**  

Dashboards include key KPIs, trends, and insights such as those displayed in your FMCG report  [oai_citation:0â€¡fmcg_dashboard.pdf](sediment://file_000000003ae0720881f45e911ad73d6d):
- Total revenue (â‚¹119.93B)
- Total quantity (39.03M units)
- Category revenue breakdown
- Customer revenue leaders
- Monthly performance trends
- Price vs Quantity visualizations (scatter bubble charts)
- Category-level revenue insights

Genie allows ad-hoc analysis using natural language:
> â€œShow revenue contribution by category in Q3.â€  
> â€œWhich customers purchased high-value products last quarter?â€  
> â€œCompare child-company sales vs parent-company sales YOY.â€

---

## ğŸš€ Key Features

### âœ” **Multi-Org Governance via Unity Catalog**
- Isolated child company catalog  
- Parent organization catalog  
- Controlled merge at the Gold layer  
- Full auditability  

### âœ” **Medallion Architecture**
- Bronze â†’ Silver â†’ Gold  
- Clean separation of concerns  
- Scalable, maintainable pipelines  

### âœ” **Automated ETL with LakeFlow**
- File-based ingestion  
- Incremental pipeline  
- Job orchestration  

### âœ” **Enterprise Analytics**
- Unified reporting  
- Self-service dashboards  
- AI-powered insights via Genie  

---

## ğŸ› ï¸ Technologies Used

| Layer | Technology |
|-------|------------|
| Storage | Amazon S3 |
| Ingestion | Databricks LakeFlow |
| Processing | Databricks Workflows, Spark SQL, PySpark |
| Governance | Unity Catalog |
| Orchestration | Delta Live Tables / LakeFlow Jobs |
| Serving | Databricks Dashboards, Genie |
| Formats | Delta Tables, Parquet, CSV |

---

## ğŸ“ˆ Example Dashboards  
The project includes sales insights and product-level analytics such as those shown in the uploaded PDF  [oai_citation:1â€¡fmcg_dashboard.pdf](sediment://file_000000003ae0720881f45e911ad73d6d):

- Revenue share by channel  
- Top products by revenue  
- Highest-performing categories  
- Price vs Quantity bubble analysis  
- Monthly revenue trend  
- KPI summary tiles  

---

## ğŸ“¦ Repository Structure
```
0_data/
â”‚
â”œâ”€â”€ 1_parent_company/
â”‚   â”œâ”€â”€ full_load/
â”‚   â””â”€â”€ incremental_load/
â”‚
â”œâ”€â”€ 2_child_company/
â”‚   â”œâ”€â”€ full_load/
â”‚   â”‚   â”œâ”€â”€ customers/
â”‚   â”‚   â”œâ”€â”€ gross_price/
â”‚   â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ products/
â”‚   â””â”€â”€ incremental_load/
â”‚
1_codes/
â”‚
â”œâ”€â”€ 1_setup/
â”‚   â”œâ”€â”€ dim_date_table_creation.ipynb
â”‚   â”œâ”€â”€ setup_catalog.ipynb
â”‚   â””â”€â”€ utilities.ipynb
â”‚
â”œâ”€â”€ 2_dimension_data_processing/
â”‚   â”œâ”€â”€ 1_customers_data_processing.ipynb
â”‚   â”œâ”€â”€ 2_products_data_processing.ipynb
â”‚   â””â”€â”€ 3_pricing_data_processing.ipynb
â”‚
â”œâ”€â”€ 3_fact_data_processing/
â”‚   â”œâ”€â”€ 1_full_load_fact.ipynb
â”‚   â””â”€â”€ 2_incremental_load_fact.ipynb
â”‚
2_dashboarding/
â”‚   â”œâ”€â”€ Dashboard_full
â”‚   â”œâ”€â”€ Dashboard_p1
â”‚   â”œâ”€â”€ Dashboard_p2
â”‚   â”œâ”€â”€ denormalise_table_query_fmcg.txt
â”‚   â”œâ”€â”€ fmcg_dashboard.pdf
â”‚   â””â”€â”€ README.md (dashboard documentation)
â”‚
resources/
â”‚   â”œâ”€â”€ databricks_project.excalidraw
â”‚   â””â”€â”€ project_architecture.png

```

# ğŸ™Œ Conclusion
This repository provides a **complete, scalable Databricks Lakehouse blueprint** for integrating an acquired companyâ€™s data into an enterprise analytics ecosystem.  
It combines:
- Medallion architecture  
- Full + incremental ingestion pipelines  
- Dimension & fact processing  
- Governance via Unity Catalog  
- Gold-layer merging  
- Enterprise dashboards  
